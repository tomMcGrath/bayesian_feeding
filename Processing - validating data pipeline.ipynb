{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the file lists (old and new pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353 total files\n",
      "161 files in old data\n",
      "254 files in new data\n",
      "62 shared files\n",
      "291 not shared files\n"
     ]
    }
   ],
   "source": [
    "old_data = 'all_data_backup/'\n",
    "new_data = 'new_all_data/'\n",
    "\n",
    "old_dir = os.walk(old_data)\n",
    "new_dir = os.walk(new_data)\n",
    "\n",
    "def get_files(walker):\n",
    "    filelist = []\n",
    "    for walk in walker:\n",
    "        filelist += walk[2]\n",
    "        \n",
    "    return filelist\n",
    "\n",
    "old_fileset = set(get_files(old_dir))\n",
    "new_fileset = set(get_files(new_dir))\n",
    "all_files = old_fileset.union(new_fileset)\n",
    "shared_files = old_fileset.intersection(new_fileset)\n",
    "missing_files = all_files - shared_files\n",
    "\n",
    "print '%d total files' %(len(all_files))\n",
    "print '%d files in old data' %(len(old_fileset))\n",
    "print '%d files in new data' %(len(new_fileset))\n",
    "print '%d shared files' %(len(shared_files))\n",
    "print '%d not shared files' %(len(missing_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYY_7.5_R_L_9_2014-10-17.B0109.CSV\n",
      "PYY_7.5_R_D_7_2015-02-09.B0107.CSV\n",
      "Lep_2.0_R_D_7_2015-02-04.B0107.CSV\n",
      "Lep_2.0_R_D_4_2015-02-03.B0104.CSV\n",
      "PYY_1.5_R_D_5_2015-02-23.B0105.CSV\n",
      "PYY_7.5_R_D_4_2015-02-09.B0104.CSV\n",
      "Lep_2.0_R_D_5_2015-02-04.B0105.CSV\n",
      "PYY_7.5_R_D_4_2015-02-17.B0104.CSV\n",
      "PYY_1.5_R_D_1_2015-02-17.B0101.CSV\n",
      "PYY_300.0_R_D_7_2015-02-23.B0107.CSV\n",
      "PYY_1.5_R_D_5_2015-02-09.B0105.CSV\n",
      "PYY_300.0_R_D_4_2015-02-09.B0104.CSV\n",
      "PYY_1.5_R_D_11_2015-02-17.B0111.CSV\n",
      "PYY_7.5_R_D_7_2015-02-17.B0107.CSV\n",
      "Lep_2.0_R_D_6_2015-02-04.B0106.CSV\n",
      "PYY_7.5_R_D_13_2015-02-17.B0113.CSV\n",
      "PYY_300.0_R_D_12_2015-02-17.B0112.CSV\n",
      "PYY_1.5_R_D_9_2015-02-09.B0109.CSV\n",
      "PYY_300.0_R_D_6_2015-02-09.B0106.CSV\n",
      "PYY_1.5_R_D_11_2015-02-09.B0111.CSV\n",
      "PYY_300.0_R_D_10_2015-02-17.B0110.CSV\n",
      "PYY_1.5_R_D_5_2015-02-17.B0105.CSV\n",
      "PYY_7.5_R_D_1_2015-02-17.B0101.CSV\n",
      "PYY_300.0_R_D_10_2015-02-09.B0110.CSV\n",
      "PYY_7.5_R_D_13_2015-02-09.B0113.CSV\n",
      "PYY_300.0_R_D_9_2015-02-23.B0109.CSV\n",
      "PYY_7.5_R_L_10_2014-10-17.B0110.CSV\n",
      "PYY_1.5_R_D_9_2015-02-17.B0109.CSV\n",
      "PYY_7.5_R_D_11_2015-02-23.B0111.CSV\n",
      "Lep_2.0_R_D_1_2015-02-03.B0101.CSV\n",
      "PYY_7.5_R_D_5_2015-02-09.B0105.CSV\n",
      "PYY_1.5_R_D_13_2015-02-09.B0113.CSV\n",
      "Lep_2.0_R_D_8_2015-02-03.B0108.CSV\n",
      "PYY_300.0_R_D_1_2015-02-09.B0101.CSV\n",
      "PYY_1.5_R_D_14_2015-02-23.B0114.CSV\n",
      "PYY_300.0_R_D_13_2015-02-17.B0113.CSV\n",
      "PYY_1.5_R_D_7_2015-02-09.B0107.CSV\n",
      "PYY_300.0_R_D_4_2015-02-23.B0104.CSV\n",
      "PYY_7.5_R_D_8_2015-02-09.B0108.CSV\n",
      "PYY_7.5_R_D_10_2015-02-09.B0110.CSV\n",
      "PYY_300.0_R_D_8_2015-02-09.B0108.CSV\n",
      "PYY_300.0_R_D_4_2015-02-17.B0104.CSV\n",
      "PYY_1.5_R_D_7_2015-02-17.B0107.CSV\n"
     ]
    }
   ],
   "source": [
    "new_missing = old_fileset - new_fileset\n",
    "for i in new_missing:\n",
    "    if i[0:6] == 'saline':\n",
    "        continue\n",
    "    if i[0:4] == 'LiCL':\n",
    "        continue\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e11f7300167c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcancel_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_errs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancel_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcancel_cutoff\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mneg_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mneg_cutoff\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutlier_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mout_cutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mcancel_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e11f7300167c>\u001b[0m in \u001b[0;36mcount_errs\u001b[0;34m(data_path, data_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_errs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnum_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_bouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "def count_errs(data_path, data_file):\n",
    "    ## \n",
    "    num_id, subject_id, mass, data = parser.parse_bouts(data_path+data_file)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    ## Clean data\n",
    "    cancel_count, data = helpers.remove_cancellations(data)\n",
    "    neg_count, data = helpers.remove_negatives(data)\n",
    "    outlier_count, data = helpers.remove_outliers(data, amt_max, dur_max, rate_max, dur_min)\n",
    "    \n",
    "    return cancel_count, neg_count, outlier_count\n",
    "\n",
    "## Set cutoffs - check against data processing notebook\n",
    "cancel_cutoff = 45\n",
    "neg_cutoff = 200\n",
    "out_cutoff = 45\n",
    "min_bouts = 5\n",
    "\n",
    "for i in new_missing:\n",
    "    data_path = 'data_to_process/'\n",
    "    data_file = i[-20:]\n",
    "    \n",
    "    cancel_count, neg_count, outlier_count = count_errs(data_path, data_file)\n",
    "    if cancel_count < cancel_cutoff and neg_count < neg_cutoff and outlier_count < out_cutoff:\n",
    "        print cancel_count, neg_count, outlier_count, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect individual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bouts_from_data as bfd\n",
    "reload(bfd)\n",
    "import CLAMS_parsers as parse\n",
    "import plot_ts\n",
    "reload(plot_ts)\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import CLAMS_parsers as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(plot_ts)\n",
    "\n",
    "def plot_file(data_path, data_file):\n",
    "    ## \n",
    "    num_id, subject_id, mass, data = parser.parse_bouts(data_path+data_file)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    ## Clean data\n",
    "    cancel_count, data = helpers.remove_cancellations(data)\n",
    "    neg_count, data = helpers.remove_negatives(data)\n",
    "    outlier_count, data = helpers.remove_outliers(data, amt_max, dur_max, rate_max, dur_min)\n",
    "    \n",
    "    ## Process the data to bots\n",
    "    bouts = bfd.get_events(data)\n",
    "    bouts = np.array(bouts)\n",
    "    bouts = bouts[:,:5]\n",
    "    \n",
    "    ## Plot bouts\n",
    "    plot_ts.ts_from_bouts(bouts)\n",
    "    return cancel_count, neg_count, outlier_count\n",
    "    \n",
    "def get_filepath(data_file):\n",
    "    filepath = data_file.split('_')[:4]\n",
    "    filepath = '_'.join(filepath) + '/'\n",
    "    return filepath\n",
    "\n",
    "amt_max = 4\n",
    "dur_max = 1000\n",
    "rate_max = 0.02\n",
    "dur_min = 4\n",
    "\n",
    "data_path = 'data_to_process/'\n",
    "data_file = '2015-03-09.B0113.CSV'\n",
    "#bout_data = np.loadtxt(bouts_file, delimiter='\\t', usecols=(0,1,2,3,4))\n",
    "cancel_count, neg_count, outlier_count = plot_file(data_path, data_file)\n",
    "print cancel_count, neg_count, outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
